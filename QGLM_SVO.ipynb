{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QGLM_SVO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Hybrid Classical-Quantum Classifiers via Stochastic Variational Optimization\n",
        "\n",
        "To run without installing additional packages:\n",
        "\n",
        "\n",
        "*   Upload to your Google Drive account;\n",
        "*   Open with Colab;\n",
        "*   From the Runtime menu in the upper left corner, execute Run all."
      ],
      "metadata": {
        "id": "gxdT9O8baaXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary dependencies and set random seed:"
      ],
      "metadata": {
        "id": "BCiM0v7pbC_q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23xzwdflaNvC"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import bernoulli, binom\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 2021\n",
        "rng = np.random.default_rng(seed)\n",
        "rng.random()"
      ],
      "metadata": {
        "id": "g9MP5oUuvcpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6984d4b1-e00f-43c5-8c72-4de4fbe33dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7569478279346672"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Bars-and-Stripes (BAS) samples [1]: \n",
        "* Data samples containing bars/stripes are labeled as positive, and the remainder of the patterns are labeled as negative."
      ],
      "metadata": {
        "id": "fw8qy2OnbeNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NumLinesStripes (a, b):\n",
        "  return pow(2,a)+pow(2,b)-2\n",
        "\n",
        "# generate lines and stripes\n",
        "def LinesStripes (a, b, c): \n",
        "  arr = np.zeros(a*b)\n",
        "  c = c+1\n",
        "  if (c<pow(2,a)): #lines\n",
        "    lines = np.zeros (a, dtype = bool)\n",
        "    for i in range (a):\n",
        "      if (c%2 == 1):\n",
        "        lines[i] = True\n",
        "      c = int(c/2)\n",
        "    index = 0\n",
        "    for element in lines:\n",
        "      if element:\n",
        "        for i in range(index, index+b):\n",
        "          arr[i] = 1\n",
        "      index = index+b\n",
        "  else: #stripes\n",
        "    c = c-pow(2,a)\n",
        "    stripes = np.zeros (b, dtype = bool)\n",
        "    for i in range (b):\n",
        "      if (c%2 == 1):\n",
        "        stripes[i] = True\n",
        "      c = int(c/2)\n",
        "    index = 0\n",
        "    for element in stripes:\n",
        "      if element:\n",
        "        for i in range(index, a*b, b):\n",
        "          arr[i] = 1\n",
        "      index = index+1\n",
        "  arr[arr == 0] = -1\n",
        "  return arr\n",
        "\n",
        "# generate random patterns\n",
        "def NonLinesStripes(a,b): \n",
        "  arr_non = np.random.randint(2, size=a*b)\n",
        "  arr_non[arr_non == 0] = -1\n",
        "  return arr_non\n",
        "\n",
        "# visualise patterns\n",
        "def PrintLinesStripes (a, b, arr): \n",
        "  print(arr)\n",
        "  print(CheckLinesStripes(np.array(arr),height,width))\n",
        "  index = 0\n",
        "  for i in range(a):\n",
        "    word = ''\n",
        "    for j in range (b):\n",
        "      if(arr[index] == 1):\n",
        "        word = word + 'X'\n",
        "      else:\n",
        "        word = word + '0'\n",
        "      index = index + 1\n",
        "    print(word)   \n",
        "\n",
        "# generate lines and stripes (main)\n",
        "def RandomLinesStripes (a, b): \n",
        "  maximum = pow(2,a)+pow(2,b)-2\n",
        "  return LinesStripes (a, b, random.randint(0,maximum))\n",
        "\n",
        "# check if a data sample contains a line or it is random for labeling\n",
        "def CheckLines(arr): \n",
        "  for row in arr:\n",
        "    ctt = row[0]\n",
        "    for element in row:\n",
        "      if(element != ctt):\n",
        "        return -1\n",
        "  return 1\n",
        "\n",
        "# check if a data sample contains a line/stripe or it is random for labeling\n",
        "def CheckLinesStripes(arr, *sizes): \n",
        "  if(sizes):\n",
        "    arr = arr.reshape(sizes)\n",
        "  return CheckLines(arr) or CheckLines(np.transpose(arr))  "
      ],
      "metadata": {
        "id": "VG8gDS1uve0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define data sample size d X d\n",
        "height = 4\n",
        "width = 4\n",
        "CDk = 1\n",
        "trainsize = NumLinesStripes(height,width)"
      ],
      "metadata": {
        "id": "Z1iheQMTve7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "labels = []\n",
        "\n",
        "# number of data samples\n",
        "samples_positive = 20\n",
        "samples_negative = 20\n",
        "\n",
        "# generate and label positive data samples\n",
        "for i in range(samples_positive):\n",
        "  arr = RandomLinesStripes (height, width)\n",
        "  label = 1\n",
        "  data.append([arr])\n",
        "  labels.append([label])\n",
        "\n",
        "# generate and label negative data samples\n",
        "for i in range(samples_negative):\n",
        "  arr = NonLinesStripes (height, width)\n",
        "  label = CheckLinesStripes(np.array(arr),height,width)\n",
        "  data.append([arr])\n",
        "  labels.append([label])"
      ],
      "metadata": {
        "id": "v1zTcH5Iv7u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape data set\n",
        "X_temp = np.reshape(data, newshape = (samples_positive + samples_negative, height*width))\n",
        "y_temp = np.reshape(labels, newshape = (samples_positive + samples_negative,))\n",
        "\n",
        "# test\n",
        "ind = np.random.randint(low=0, high=X_temp.shape[0], size=6)"
      ],
      "metadata": {
        "id": "qWCWWYuvwFKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "def get_idx(rng):\n",
        "    ind = np.random.randint(low=0, high=X_temp.shape[0], size=6)\n",
        "    print(ind)\n",
        "\n",
        "get_idx(rng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MccHtbk8akco",
        "outputId": "0ea5999e-aab2-4295-a92d-5d286ea66cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23 17  6  1 10 33]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-hh4uObh3x6"
      },
      "source": [
        "alpha = 0.5 # constant for moving averages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the hybrid classifier:"
      ],
      "metadata": {
        "id": "-bSCQFLicyoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class my_NN(object):\n",
        "    def __init__(self):\n",
        "        self.input = height*width # data sample dimension (flattened)\n",
        "        self.output = 1 # output dimension\n",
        "        self.hidden_units1 = 32 # single layer\n",
        "\n",
        "        self.normalising_constant = self.hidden_units1 # normalising constant\n",
        "\n",
        "        # initialize matrix of parameters;\n",
        "        # variational parameters: input -> hidden layer \n",
        "        self.p1 = np.zeros([self.input, self.hidden_units1]) # input X hidden_units matrix\n",
        "        self.synaptic_params1 = self._sigmoid(self.p1, 1)\n",
        "        \n",
        "        # variational parameters: hidden layer -> output\n",
        "        self.p2 = np.zeros([self.hidden_units1, self.output])  # hidden_units X output matrix\n",
        "        self.synaptic_params2 = self._sigmoid(self.p2, 1)\n",
        "        \n",
        "        \n",
        "        # weights1: input -> hidden layer \n",
        "        self.w1 = self.get_weights(self.synaptic_params1) # input X hidden_units matrix\n",
        "        # weights2: hidden layer -> hidden layer\n",
        "        self.w2 = self.get_weights(self.synaptic_params2)  # hidden_units X output matrix\n",
        "\n",
        "        self.baseline1 = 0 #initialise baseline for variational parameters\n",
        "        self.baseline2 = 0 #initialise baseline for variational parameters\n",
        "\n",
        "        self.e1_prev = np.zeros_like(self.w1) #initialise previous gradient for variational parameters (for moving average)\n",
        "        self.e2_prev = np.zeros_like(self.w2) #initialise previous gradient for variational parameters (for moving average)\n",
        "\n",
        "        \n",
        "\n",
        "        self.loss_prev = 0\n",
        "\n",
        "        self.loss_list = []\n",
        "\n",
        "        self.flag = 0\n",
        "\n",
        "    \n",
        "    # sample weights/outputs - training\n",
        "    def get_weights(self, synaptic_params):\n",
        "        synaptic_params[synaptic_params > 0.9999] = 1 # depending on numerical precision, increse/decrese threshold to avoid probability greater than one\n",
        "        synaptic_params[synaptic_params < 0] = 0 # depending on numerical precision, increse/decrese threshold to avoid probability lesser than zero\n",
        "        self.weights = np.random.binomial(n = 1, p = synaptic_params, size = synaptic_params.shape) # sample weights\n",
        "        self.weights[self.weights == 0] = -1 # np.random.binomial returns zeroes and ones; replace zeros with minus ones\n",
        "\n",
        "        return self.weights\n",
        "\n",
        "    # sample weights/outputs - inference\n",
        "    def get_weights_eval(self, synaptic_params, threshold):\n",
        "        synaptic_params[synaptic_params > 0.9999] = 1 # depending on numerical precision, increse/decrese threshold to avoid probability greater than one\n",
        "        synaptic_params[synaptic_params < 0] = 0 # depending on numerical precision, increse/decrese threshold to avoid probability lesser than zero\n",
        "        self.weights = np.where(synaptic_params > threshold, 1, -1) # set weights using a fixed threshold\n",
        "\n",
        "        return self.weights\n",
        "\n",
        "    # QGLM response function\n",
        "    def get_response(self, dot):\n",
        "        self.response = (1/2) + (1/2)*((1/self.normalising_constant)*(dot))**2  # Biased Quadratic; \n",
        "                                                                                # see Biased_Quadratic_Qiskit.ipynb for an implementation routine on quantum hardware\n",
        "\n",
        "        return self.response\n",
        "\n",
        "    # CGLM response function\n",
        "    def get_response_classical(self, dot):\n",
        "        self.response = self._sigmoid_classical(dot) \n",
        "\n",
        "        return self.response\n",
        "    \n",
        "    # forward propagation - training\n",
        "    def _forward_propagation(self, X): \n",
        "        self.z2 = np.dot(self.w1.T, X.T) # multiply weights and input\n",
        "        self.a2_temp = self.get_response(self.z2) # get QGLM response \n",
        "        self.a2 = self.get_weights(self.a2_temp) # sample QGLM output - same function used for sampling weights with the response as input \n",
        "        self.a2[self.a2 == 0] = -1 # np.random.binomial returns zeroes and ones; replace zeros with minus ones\n",
        "\n",
        "        self.z3 = np.dot(self.w2.T, self.a2) # multiply weights and output of hidden layer\n",
        "        self.a3_temp = self.get_response_classical(self.z3) # get CGLM response \n",
        "\n",
        "        return self.a3_temp\n",
        "\n",
        "    # forward propagation - inference\n",
        "    def _forward_propagation_eval(self, X, threshold): \n",
        "        self.z2 = np.dot(self.w1.T, X.T) # multiply weights and input\n",
        "        self.a2_temp = self.get_response(self.z2) # get QGLM response \n",
        "        self.a2 = self.get_weights_eval(self.a2_temp, threshold) # get QGLM output\n",
        "        self.a2[self.a2 == 0] = -1 # replace zeros with minus ones \n",
        "\n",
        "        self.z3 = np.dot(self.w2.T, self.a2) # multiply weights and output of hidden layer\n",
        "        self.a3_temp = self.get_response_classical(self.z3) # get CGLM response \n",
        "\n",
        "        return self.a3_temp\n",
        "\n",
        "    # tempered sigmoid with temperature annealing schedule \n",
        "    def _sigmoid(self, z, epoch):\n",
        "        initial_temp = 1 \n",
        "        temperature = initial_temp\n",
        "        min = 0.01 \n",
        "        epochs_decrease = 200 \n",
        "        if epoch > 0 and epoch%epochs_decrease  == 0:\n",
        "            if 1 - initial_temp * (min*(epoch/epochs_decrease)) > min:\n",
        "                temperature = 1 - initial_temp * (min*(epoch/epochs_decrease))\n",
        "            else:\n",
        "                temperature = min\n",
        "\n",
        "        return 1/(1+np.exp(-z/temperature))\n",
        "\n",
        "    # classical sigmoid\n",
        "    def _sigmoid_classical(self, z):\n",
        "\n",
        "        return 1/(1+np.exp(-z))\n",
        "\n",
        "    # loss function\n",
        "    def _loss(self, predict, y):\n",
        "        const = 0.001 # depending on numerical precision, increse/decrese const to avoid log of zero\n",
        "        m = y.shape[0]\n",
        "        predict[predict > 0.9999] = 1\n",
        "        logprobs = np.multiply(np.log(predict + const), ((1 + y)/2)) + np.multiply(((1 - y)/2), np.log(1 - predict + const))\n",
        "        loss = - np.sum(logprobs) / m\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # backward propagation\n",
        "    def _backward_propagation(self, X, y, loss, i):\n",
        "        self.dp1_temp = self._loss_prime(self.w1, self.p1, i)\n",
        "        self.dp2_temp = self._loss_prime(self.w2, self.p2, i)\n",
        "        \n",
        "    # loss derivative\n",
        "    def _loss_prime(self, w, p, i):\n",
        "\n",
        "        return ((1+w)/2 - self._sigmoid(p, i))\n",
        "\n",
        "    # update parameters\n",
        "    def _update(self, loss, num_samples, i):\n",
        "        default_learning_rate = 0.2\n",
        "        learning_rate = self.step_decay(i)\n",
        "        self.p1 = self.p1 - default_learning_rate*((learning_rate*(self.loss - self.baseline1))*self.e1/num_samples)\n",
        "        self.p2 = self.p2 - default_learning_rate*((learning_rate*(self.loss - self.baseline2))*self.e2/num_samples)\n",
        "\n",
        "    # update baseline with moving average \n",
        "    def _update_baseline(self, loss, num_samples, i):\n",
        "        const = 0.0001\n",
        "        self.baseline1 = (alpha*self.loss_prev*(self.e1_prev**2) + (1-alpha)*self.loss*((self.e1/num_samples)**2)) / ((alpha*(self.e1_prev**2) + (1-alpha)*((self.e1/num_samples)**2))+ const)\n",
        "        self.baseline2 = (alpha*self.loss_prev*(self.e2_prev**2) + (1-alpha)*self.loss*((self.e2/num_samples)**2)) / (alpha*(self.e2_prev**2) + (1-alpha)*((self.e2/num_samples)**2)+ const)\n",
        "        \n",
        "\n",
        "    # cyclical learning rate schedule\n",
        "    def step_decay(self, epoch):\n",
        "        base_lr = 0.1\n",
        "        max_lr = 1000\n",
        "        step_size = 1000\n",
        "        cycle = np.floor(1+epoch/(2*step_size))\n",
        "        x = np.abs(epoch/step_size - 2*cycle + 1)\n",
        "        lrate = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))*0.5*(1+np.sin(cycle*np.pi/2.))\n",
        "\n",
        "        return lrate\n",
        "\n",
        "    # training function\n",
        "    def train(self, Xtrain, ytrain, iteration=5000, num_samples = 10, batch_size = 16): \n",
        "\n",
        "        ###################### Training ########################\n",
        "\n",
        "        if self.flag == 0:\n",
        "\n",
        "            for i in range(iteration):\n",
        "\n",
        "                ind = np.random.randint(low=0, high=X_temp.shape[0], size=batch_size) # create random batch of samples\n",
        "                X = Xtrain[ind]\n",
        "                y = ytrain[ind]\n",
        "\n",
        "                loss_avg = 0\n",
        "\n",
        "                self.e1 = np.zeros_like(self.w1) \n",
        "                self.e2 = np.zeros_like(self.w2) \n",
        "\n",
        "                # use multiple samples of the variational distribution q(.|.) to estimate loss\n",
        "                for j in range(num_samples):\n",
        "                    self.synaptic_params1 = self._sigmoid(self.p1, i) # weight probabilities\n",
        "                    self.synaptic_params2 = self._sigmoid(self.p2, i) # weight probabilities\n",
        "\n",
        "                    self.w1 = self.get_weights(self.synaptic_params1) # sample weights\n",
        "                    self.w2 = self.get_weights(self.synaptic_params2) # sample weights\n",
        "\n",
        "                    y_hat = self._forward_propagation(X) # forward propagation\n",
        "                    loss = self._loss(y_hat, y) # calculate loss\n",
        "\n",
        "                    loss_avg = loss_avg + loss # sum of loss over samples of the variational distribution q(.|.)\n",
        "\n",
        "                    self._backward_propagation(X, y, loss_avg/num_samples, i) # backward propagation\n",
        "\n",
        "                    self.e1 = self.e1 + self.dp1_temp # sum of gradient over samples of the variational distribution q(.|.)\n",
        "                    self.e2 = self.e2 + self.dp2_temp # sum of gradient over samples of the variational distribution q(.|.)\n",
        "\n",
        "                self.loss = loss_avg/num_samples\n",
        "\n",
        "                self._update_baseline(self.loss, num_samples, i) # update the baseline\n",
        "                self._update(self.loss, num_samples, i) # update the variational parameters\n",
        "\n",
        "                self.e1_prev = self.e1/num_samples\n",
        "                self.e2_prev = self.e2/num_samples\n",
        "\n",
        "                self.loss_prev = self.loss\n",
        "\n",
        "                ###################### Inference ########################\n",
        "\n",
        "                if i == 0 or i%1000==0:\n",
        "\n",
        "                    if i == 0:\n",
        "                        Wthresholds = [0.5] # set initial threshold \n",
        "                    else:\n",
        "                        Wthresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9] # optimize output threshold during training\n",
        "\n",
        "                    score_best = 0\n",
        "\n",
        "                    # find the best output threshold\n",
        "                    for Wthreshold in Wthresholds:\n",
        "\n",
        "                        self.synaptic_params1 = self._sigmoid(self.p1, i)\n",
        "                        self.synaptic_params2 = self._sigmoid(self.p2, i)\n",
        "\n",
        "                        self.w1 = self.get_weights_eval(self.synaptic_params1, Wthreshold) \n",
        "                        self.w2 = self.get_weights_eval(self.synaptic_params2, Wthreshold) \n",
        "\n",
        "                        pred_avg = np.zeros_like(y_temp) \n",
        "                        loss_eval = 0\n",
        "                        for j in range(num_samples):\n",
        "\n",
        "                            y_hat = self._forward_propagation(Xtrain)\n",
        "                            l = self._loss(y_hat, ytrain)\n",
        "\n",
        "                            pred_avg = pred_avg + y_hat\n",
        "\n",
        "                            loss_eval = loss_eval + l\n",
        "\n",
        "                        self.loss_list.append(loss_eval/num_samples)\n",
        "                    \n",
        "\n",
        "                        y_hat_test = pred_avg/num_samples\n",
        "\n",
        "                        # use the geometric mean\n",
        "                        fpr, tpr, thresholds = roc_curve(y_temp, y_hat_test.reshape(samples_positive + samples_negative,))\n",
        "                        gmeans = (tpr*samples_positive + (1-fpr)*samples_negative)/(samples_positive + samples_negative)\n",
        "                        ix = np.argmax(gmeans)\n",
        "\n",
        "                        y_hat_test_roc = [1 if i[0] > thresholds[ix] else -1 for i in y_hat_test.T] \n",
        "                        \n",
        "                        score = self.score(y_hat_test_roc, y_temp) # get the accuracy score\n",
        "\n",
        "                        \n",
        "                        if score > score_best:\n",
        "                            score_best = score\n",
        "                    \n",
        "                    print(\"Iteration and score: \", i, score_best)\n",
        "\n",
        "                    \n",
        "\n",
        "\n",
        "    def predict_test(self, X):\n",
        "        y_hat_response = self._forward_propagation(X)\n",
        "        y_hat = y_hat_response \n",
        "        return np.array(y_hat)\n",
        "    \n",
        "    # get the accuracy score \n",
        "    def score(self, predict, y):\n",
        "        cnt = np.sum(predict==y)\n",
        "        return (cnt/len(y))*100\n",
        "        "
      ],
      "metadata": {
        "id": "1l33jPHT2Z8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function; initialize and train the model."
      ],
      "metadata": {
        "id": "A8zI3vT6z2LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    clr = my_NN() #initialize the model\n",
        "    clr.train(X_temp, y_temp) # train model "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaHcG4SW2aJl",
        "outputId": "149d82f4-511e-4af3-9a0f-5ec419475f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration and score:  0 57.49999999999999\n",
            "Iteration and score:  1000 67.5\n",
            "Iteration and score:  2000 67.5\n",
            "Iteration and score:  3000 70.0\n",
            "Iteration and score:  4000 70.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "[1]  F. Tacchino, C. Macchiavello, D. Gerace, and D. Bajoni, *An artificial neuron  implemented  on  an  actual  quantum  processor,* npj  QuantumInformation, vol. 5, no. 1, pp. 1–8, 2019."
      ],
      "metadata": {
        "id": "uZf1K8E1VEEi"
      }
    }
  ]
}